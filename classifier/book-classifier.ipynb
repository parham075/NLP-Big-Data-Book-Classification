{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# The Prediction Task\n",
    "In this notebook, we will be using the dataset gathered by our [data scraping script](https://github.com/shkhaksar/book-classification) merged with a dataset from [Kaggle](https://www.kaggle.com/mdhamani/goodreads-books-100k).\n",
    "The Kaggle dataset is transformed to our dataset before merge. for more information you can see `merger.py`\n",
    "\n",
    "The dataset contains book summaries for 101K books extracted from [Goodreads](https://www.goodreads.com/) website.\n",
    "\n",
    "The classification goal is, given a new book, to predict the book belongs to which category of books.\n",
    "\n",
    "## 1. Data Acquisition\n",
    "This is the first step we need to accomplish before going any further. The dataset will be downloaded and loaded to DBFS."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%sh wget -P /tmp https://github.com/shkhaksar/book-classification/raw/master/classifier/dataset.csv.bz2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%fs` not found.\n"
     ]
    }
   ],
   "source": [
    "%fs ls file: /tmp/"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Data Exploration\n",
    "A set of plots to show why our data is unbiased\n",
    "\n",
    "1. Histograms of individual categorical features"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#code here"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "2. Relationship between categorical features and the target variable (deposit)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#code here"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Data Preprocessing\n",
    "\n",
    "## 3.1 Balanced vs. Unbalanced Dataset\n",
    "Let's first verify our dataset is actually balanced.\n",
    "Dataset Splitting: Training vs. Test Set"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. Feature Engineering (TF-IDF) [TODO: explain why TF-IDF and then learn with TF-IDF]\n",
    "Machine learning techniques cannot work directly on text data; in fact, words must be first converted into some numerical representation that machine learning algorithms can make use of. This process is often known as vectorization.\n",
    "\n",
    "In terms of vectorization, it is important to remember that it isn't merely turning a single word into a single number. While words can be transformed into numbers, an entire document can be translated into a vector. Moreover, vectors derived from text data are usually high-dimensional. This is because each dimension of the feature space will correspond to a word, and the language in the documents may have thousands of words."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. Feature Learning (instead of TF-IDF) [TODO: try with Word2Vec (or similar method) and explain why it's better]\n",
    "Word2Vec computes distributed vector representation of words. The main advantage of the distributed representations is that similar words are close in the vector space, which makes generalization to novel patterns easier and model estimation more robust. Distributed vector representation is showed to be useful in many natural language processing applications such as named entity recognition, disambiguation, parsing, tagging and machine translation.\n",
    "\n",
    "## 5. Model\n",
    "\n",
    "## 5.1 Logistic Regression (1st model)\n",
    "Train and evaluate on the test set using Evaluate model performance on the Test Set\n",
    "\n",
    "## 5.2 Another Model"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}